{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The OpenSource TestPlan Playbook\n",
    "_[Based on gkamradt's Ask A Book Questions](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/Ask%20A%20Book%20Questions.ipynb)_\n",
    "\n",
    "This Jupyter notebook is an example on how we can load data into a vector database to provide context to a large language model.\n",
    "LLMs are hard to avoid these days and they can be really useful tools to get your answers when discovering a new library.\n",
    "The idea here was to create a POC/play around with OpenAI for the opensource TestPlan documentation, so we can chat with it. LLMs can be used to summarize long texts, but there is a upper limit on how long the input text can be. \n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "Fortunately, we can \"expand\" its knowledge. We do not need to train a LLM to get specific context. We can use OpenAI Embeddings to create vectors for our data, which the gpt api can understand and search in. Simply, we convert our data and query into vectors of 1536. Vector databases provide a way to do a similarity search based on these values. We can get the most similar ones to the query, pass that to the GPT API as context, which it should use to answer our question.\n",
    "\n",
    "**Want to try it out?**\n",
    "\n",
    "- You will need an OpenAI API key. Make sure to set a monthly limit on your usage, so you can still pay your rent. Jokes aside. Embedding the whole documentation cost less than a dollar. BUT be sure to set that usage limit. The text completion costs significantly more though\n",
    "- You will need a Pinecone account. There you want to create a index called 'testplan' with the size of 1536. This is the size that OpenAI uses.\n",
    "\n",
    "\n",
    "_Make sure to not share your API Keys with anyone_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urldefrag\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scraping the URLs from the OpenSource TetsPlan Documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href:\n",
    "            yield urljoin(url, href)\n",
    "\n",
    "def is_download_link(url, base_url):\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.path.startswith(\"/_downloads/\") or parsed.path.endswith(\".py\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def main(base_url):\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = {base_url}\n",
    "    all_urls = set()\n",
    "\n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop()\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url}\")\n",
    "        visited_urls.add(current_url)\n",
    "\n",
    "        for link in get_all_links(current_url):\n",
    "            link_no_fragment, _ = urldefrag(link)\n",
    "            if link_no_fragment.startswith(base_url) and not is_download_link(link_no_fragment, base_url) and link_no_fragment.endswith('.html'):\n",
    "                all_urls.add(link_no_fragment)\n",
    "                urls_to_visit.add(link_no_fragment)\n",
    "\n",
    "    print(\"\\nAll URLs:\")\n",
    "    for url in all_urls:\n",
    "        print(url)\n",
    "    \n",
    "    return all_urls\n",
    "\n",
    "base_url = \"https://testplan.readthedocs.io/en/latest/\"\n",
    "all_urls= main(base_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape the data and store it in raw documents from the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(urls=all_urls)\n",
    "raw_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '...'\n",
    "PINECONE_API_KEY = '...'\n",
    "PINECONE_API_ENV = '...'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the OpenAIEmbedding so that we can create the Vectors for each text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "index_name = \"testplan\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embed the chunks and push them into the Pinecone database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ask the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How to create a new driver for a custom application?'\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0,openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type='stuff')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can run tests interactively by running your testplan with the \"-i\" flag. This will start up a web UI that you can access at http://localhost:39514/interactive/. This will allow you to run individual testcases and testsuites on-demand and control test environments on-demand.\n"
     ]
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Testplan is tested to work with Python 3.7 and 3.8, 3.10 and 3.11 so we recommend choosing one of those.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What python versions are supported by TestPlan?'\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You can make parameterized tests by passing a dictionary of lists/tuples or list of dictionaries/tuples as parameters value to the testcase method declaration. See the downloadable example for more information.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'How can you make parameterized tests?'\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
